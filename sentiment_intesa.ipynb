{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_intesa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnh3jZoDweU0sIyBKWRYEJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usesnames/Reviews_analyzer/blob/main/sentiment_intesa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu9LJQY-ghMm",
        "outputId": "e3345e8a-5043-42f0-a11b-76de54b2e464"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igcJzn879FXX"
      },
      "source": [
        "# Acquisizione dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we0MhlUF_-uO"
      },
      "source": [
        "!pip install -qq google-play-scraper\n",
        "#!pip install app-store-scraper\n",
        "from google_play_scraper import Sort, reviews, app, reviews_all\n",
        "app_name = 'com.latuabancaperandroid'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcqDfOxyNWrA"
      },
      "source": [
        "Helper function that prints JSON objects a bit better:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4n-vkhrZJPp_"
      },
      "source": [
        "def print_json(json_object):\n",
        "  json_str = json.dumps(\n",
        "    json_object, \n",
        "    indent=2, \n",
        "    sort_keys=True, \n",
        "    default=str\n",
        "  )\n",
        "  print(json_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRIID99ZCfjv"
      },
      "source": [
        "app_reviews = []\n",
        "\n",
        "for score in list(range(1, 6)):\n",
        "  rvs, _ = reviews(\n",
        "    app_name,\n",
        "    lang='it',\n",
        "    country='it',\n",
        "    sort=Sort.NEWEST,\n",
        "    count= 4000, \n",
        "    filter_score_with=score\n",
        "  )\n",
        "  for r in rvs:\n",
        "    r['appId'] = app_name\n",
        "  app_reviews.extend(rvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF5exqgWCh1Q"
      },
      "source": [
        "print_json(app_reviews[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2YjMDQDOdC5"
      },
      "source": [
        "len(app_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKQ79Xhuigy4"
      },
      "source": [
        "# Parametri"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LHqA9RWilu6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvA4OXZR9MzL"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L7Nq2uwCjRI"
      },
      "source": [
        "if not os.path.isfile('balanced_reviews.csv'):\n",
        "  balanced_reviews_df = pd.DataFrame(app_reviews)\n",
        "  balanced_reviews_df.to_csv('balanced_reviews.csv', index=None, header=True, sep='\\t')\n",
        "  print('generato file balanced_reviews.csv')\n",
        "\n",
        "balanced_reviews_df = pd.read_csv('balanced_reviews.csv', sep='\\t')\n",
        "balanced_reviews_df.drop(labels=['userName', 'userImage', 'replyContent', 'repliedAt', \n",
        "                                 'at', 'appId', 'reviewCreatedVersion', 'reviewId'], \n",
        "                         axis=1, inplace=True)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbWj1xNg2Jd"
      },
      "source": [
        "def remove_stop_words(dataset):\n",
        "    stop_words = set(stopwords.words('italian'))\n",
        "    for i in dataset:\n",
        "        i = ([token.lower() for token in i if token not in stop_words])\n",
        "    return dataset\n",
        "\n",
        "def normalize(dataset):\n",
        "    lemmatizer = nltk.stem.snowball.ItalianStemmer(ignore_stopwords=False)\n",
        "    for i in dataset:\n",
        "        i = \" \".join([lemmatizer.stem(token) for token in i]).strip()\n",
        "    return dataset\n",
        "\n",
        "def remove_garbage(dataset):\n",
        "    garbage = \"~`!@#$%^&*()_-+={[}]|\\:;'<,>.?/\"\n",
        "    for i in dataset:\n",
        "      i = \"\".join([char for char in i if char not in garbage])\n",
        "    return dataset\n",
        "\n",
        "def fit_corpus(train_data):\n",
        "    tfidf = TfidfVectorizer(min_df=10, max_df=0.6, ngram_range=(1,2))\n",
        "    tfidf.fit(train_data)\n",
        "    return tfidf\n",
        "\n",
        "def transform_data(tfidf, dataset):\n",
        "    features = tfidf.transform(dataset)\n",
        "    return pd.DataFrame(features.todense(), columns = tfidf.get_feature_names())"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyMif9L0Kx5u"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(balanced_reviews_df['content'], balanced_reviews_df['score'], \n",
        "                                                    test_size=0.3, random_state=42, stratify=balanced_reviews_df['score'])\n",
        "y_train = y_train.map(lambda num_stars: 1 if (num_stars == 5 or num_stars == 4) else 0)\n",
        "y_test = y_test.map(lambda num_stars: 1 if (num_stars == 5 or num_stars == 4) else 0)\n",
        "\n",
        "def preprocess(data, tfidf=None):\n",
        "  data.map(lambda el: nltk.tokenize.word_tokenize(el))\n",
        "  data = remove_stop_words(data)\n",
        "  data = normalize(data)\n",
        "  data = remove_garbage(data)\n",
        "  if tfidf==None:\n",
        "    tfidf = fit_corpus(data) #Fitting the vecorizer\n",
        "  features = transform_data(tfidf, data)\n",
        "  return features, tfidf"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZm_odW0lUwY"
      },
      "source": [
        "X_train.reset_index(inplace=True, drop=True)\n",
        "X_test.reset_index(inplace=True, drop=True)\n",
        "train_features, tfidf = preprocess(X_train)  #transforming \n",
        "test_features, _ = preprocess(X_test, tfidf)    "
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-z4HEg7fpvb"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoN65RHzmkum",
        "outputId": "0c27ce1e-1951-4c8a-b6bd-937f7b02e554"
      },
      "source": [
        "clf = LogisticRegression(random_state=0, solver='lbfgs', C=0.6, tol=0.0001, max_iter=1000)\n",
        "clf.fit(train_features, y_train)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.6, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSB3sM2Smu7F",
        "outputId": "87b92dd9-9ec9-4c4a-9813-6e4a6e7be385"
      },
      "source": [
        "clf.score(test_features, y_test)"
      ],
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8841666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGGnIYyWm2Z_",
        "outputId": "0d9a31ce-90d3-46d1-fea1-634a1da3699a"
      },
      "source": [
        "demo_review = \"bella e utile, ma troppi crash\"\n",
        "\n",
        "prediction = clf.predict(preprocess(pd.Series([demo_review]), tfidf)[0])[0]\n",
        "if prediction==1:\n",
        "  print(\"recensione postiva\")\n",
        "else:\n",
        "  print(\"recensione non positiva\")"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "recensione non positiva\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA0Dk72sfvBS"
      },
      "source": [
        "#Serialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kkjUtQEYTb_",
        "outputId": "056d7a7f-b951-40a5-93a7-f96fd54cd6b2"
      },
      "source": [
        "# create a TF model with the same architecture\n",
        "tf_model = tf.keras.models.Sequential()\n",
        "tf_model.add(tf.keras.Input(shape=(len(tfidf.vocabulary_),)))\n",
        "tf_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# assign the parameters from sklearn to the TF model\n",
        "tf_model.layers[0].weights[0].assign(clf.coef_.transpose())\n",
        "tf_model.layers[0].bias.assign(clf.intercept_)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(1,) dtype=float32, numpy=array([-0.22950141], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-kZNd_VYslG",
        "outputId": "b0cbdbae-4a56-481c-ef22-94baf13f7045"
      },
      "source": [
        "predictions = clf.predict(preprocess(X_test, tfidf)[0])\n",
        "tf_predictions = tf_model(np.array(preprocess(X_test, tfidf)[0])) \n",
        "\n",
        "flag=\"stesse predictions\"\n",
        "for el, tf_el in zip(predictions, tf_predictions.numpy()):\n",
        "  if((el==1 and tf_el[0]<0.5) or (el==0 and tf_el[0]>0.5)):\n",
        "    flag=\"predicitons divergono\"\n",
        "print(flag)"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "stesse predictions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ2QhckQeoVr",
        "outputId": "ea53a98c-12fb-4993-958e-9a00b7e587fc"
      },
      "source": [
        "tf_model.save('saved_model')\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model') \n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('sentiment_model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}